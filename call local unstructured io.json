{"name":"call local unstructured io","webhook":false,"icon":null,"icon_bg_color":null,"endpoint_name":null,"folder_id":"6b34c904-1ef4-49f7-bde2-329b1a04166b","gradient":null,"id":"6619926c-9f16-4f75-8363-80d9877ec48b","is_component":false,"data":{"nodes":[{"id":"CustomComponent-CrbIY","type":"genericNode","position":{"x":-362.45164531095276,"y":-118.59914095481737},"data":{"type":"Unstructured","node":{"template":{"_type":"Component","file":{"trace_as_metadata":true,"file_path":"6619926c-9f16-4f75-8363-80d9877ec48b/2024-12-12_10-57-34_testtest.pdf","fileTypes":["pdf","docx","txt"],"list":false,"required":true,"placeholder":"","show":true,"name":"file","value":"testtest.pdf","display_name":"File","advanced":false,"dynamic":false,"info":"The path to the file with which you want to use Unstructured to parse. Supports: PDF, DOCX, TXT","title_case":false,"type":"file","_input_type":"FileInput","load_from_db":false},"api_url":{"load_from_db":false,"required":true,"placeholder":"","show":true,"name":"api_url","value":"http://192.168.0.36:8080/general/v0/general","display_name":"Unstructured.io API URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Custom Unstructured API URL (e.g., http://localhost:8000/general/v0/general)","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\r\nimport requests\r\nfrom langflow.custom import Component\r\nfrom langflow.inputs import FileInput, SecretStrInput\r\nfrom langflow.schema import Data\r\nfrom langflow.template import Output\r\n\r\nclass UnstructuredComponent(Component):\r\n    display_name = \"Unstructured\"\r\n    description = \"Uses Unstructured.io to extract clean text from raw source documents. Supports: PDF, DOCX, TXT\"\r\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/providers/unstructured/\"\r\n    trace_type = \"tool\"\r\n    icon = \"Unstructured\"\r\n    name = \"Unstructured\"\r\n    inputs = [\r\n        FileInput(\r\n            name=\"file\",\r\n            display_name=\"File\",\r\n            required=True,\r\n            info=\"The path to the file with which you want to use Unstructured to parse. Supports: PDF, DOCX, TXT\",\r\n            file_types=[\"pdf\", \"docx\", \"txt\"],\r\n        ),\r\n        SecretStrInput(\r\n            name=\"api_url\",\r\n            display_name=\"Unstructured.io API URL\",\r\n            required=True,\r\n            info=\"Custom Unstructured API URL (e.g., http://localhost:8000/general/v0/general)\",\r\n        ),\r\n    ]\r\n    outputs = [\r\n        Output(name=\"data\", display_name=\"Data\", method=\"load_documents\"),\r\n    ]\r\n    \r\n    def build_unstructured(self) -> requests.Response:\r\n        # Get file name and content type\r\n        file_name = os.path.basename(self.file)\r\n        file_extension = os.path.splitext(file_name)[1].lower()\r\n        \r\n        # Determine content type\r\n        content_type = {\r\n            '.pdf': 'application/pdf',\r\n            '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\r\n            '.txt': 'text/plain'\r\n        }.get(file_extension, 'application/octet-stream')\r\n        \r\n        # Read file in binary mode\r\n        with open(self.file, 'rb') as f:\r\n            file_content = f.read()\r\n        \r\n        # Use requests' files parameter for file upload\r\n        files = {\r\n            'files': (file_name, file_content, content_type)\r\n        }\r\n        \r\n        # Send POST request\r\n        try:\r\n            response = requests.post(\r\n                self.api_url, \r\n                files=files,\r\n                timeout=30  # Set timeout\r\n            )\r\n            return response\r\n        except requests.RequestException as e:\r\n            raise Exception(f\"API request error: {str(e)}\")\r\n    \r\n    def load_documents(self) -> list[Data]:\r\n        # Call API and process response\r\n        response = self.build_unstructured()\r\n        \r\n        # Check response status code\r\n        if response.status_code == 200:\r\n            try:\r\n                # Parse JSON response\r\n                documents = response.json()\r\n                \r\n                # Create Data objects with explicit metadata handling\r\n                data = []\r\n                for doc in documents:\r\n                    # Ensure metadata is a dictionary\r\n                    metadata = doc.get('metadata', {}) if isinstance(doc, dict) else {}\r\n                    \r\n                    # Create Data object with text and metadata\r\n                    data_item = Data(\r\n                        text=doc.get('text', ''),  # Ensure text is extracted\r\n                        metadata=metadata\r\n                    )\r\n                    data.append(data_item)\r\n                \r\n                self.status = data\r\n                return data\r\n            except ValueError as e:\r\n                raise Exception(f\"JSON parsing error: {str(e)}\")\r\n        else:\r\n            # Handle error response\r\n            raise Exception(f\"Unstructured API error: {response.status_code} - {response.text}\")","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false}},"description":"Uses Unstructured.io to extract clean text from raw source documents. Supports: PDF, DOCX, TXT","icon":"Unstructured","base_classes":["Data"],"display_name":"Custom Component","documentation":"https://python.langchain.com/v0.2/docs/integrations/providers/unstructured/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"load_documents","value":"__UNDEFINED__","cache":true}],"field_order":["file","api_url"],"beta":false,"edited":true,"metadata":{},"lf_version":"1.0.19.post2"},"id":"CustomComponent-CrbIY"},"selected":false,"width":384,"height":405,"positionAbsolute":{"x":-362.45164531095276,"y":-118.59914095481737},"dragging":false},{"id":"ParseData-MJ5EY","type":"genericNode","position":{"x":208.5159413153416,"y":16.499854752372883},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data","value":"","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sep","value":"\n","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{text}","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"ParseData-MJ5EY"},"selected":false,"width":384,"height":351,"positionAbsolute":{"x":208.5159413153416,"y":16.499854752372883},"dragging":false},{"id":"ChatOutput-ToGpZ","type":"genericNode","position":{"x":1502.7389132562084,"y":662.5143619431938},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"ChatOutput-ToGpZ"},"selected":true,"width":384,"height":287,"positionAbsolute":{"x":1502.7389132562084,"y":662.5143619431938},"dragging":false},{"id":"Prompt-oCI53","type":"genericNode","position":{"x":117.02454720765036,"y":565.6291881809184},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"This is the content of the file analyzed by the user in unstructured io. \nPlease output the analysis content directly first, and then check whether there is any need to modify, try not to modify unless there is a serious error.\nuser input: {user_input}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"user_input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user_input","display_name":"user_input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_input"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"error":null,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"Prompt-oCI53"},"selected":false,"width":384,"height":389,"positionAbsolute":{"x":117.02454720765036,"y":565.6291881809184},"dragging":false},{"id":"GoogleGenerativeAIModel-lXKNn","type":"genericNode","position":{"x":733.7180585757278,"y":799.0908444522523},"data":{"type":"GoogleGenerativeAIModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-1.0-pro-vision\"],\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key),\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"google_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"google_api_key","value":null,"display_name":"Google API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_output_tokens","value":0,"display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model":{"trace_as_metadata":true,"options":["gemini-1.5-pro","gemini-1.5-flash","gemini-1.0-pro","gemini-1.0-pro-vision"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model","value":"gemini-1.5-pro","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["google_api_key","max_output_tokens","model","n","temperature","top_k","top_p"]}],"field_order":["input_value","system_message","stream","max_output_tokens","model","google_api_key","top_p","temperature","n","top_k","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19.post2"},"id":"GoogleGenerativeAIModel-lXKNn"},"selected":false,"width":384,"height":669,"dragging":false,"positionAbsolute":{"x":733.7180585757278,"y":799.0908444522523}}],"edges":[{"source":"CustomComponent-CrbIY","sourceHandle":"{œdataTypeœ:œUnstructuredœ,œidœ:œCustomComponent-CrbIYœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"ParseData-MJ5EY","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-MJ5EYœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-MJ5EY","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Unstructured","id":"CustomComponent-CrbIY","name":"data","output_types":["Data"]}},"id":"reactflow__edge-CustomComponent-CrbIY{œdataTypeœ:œUnstructuredœ,œidœ:œCustomComponent-CrbIYœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-ParseData-MJ5EY{œfieldNameœ:œdataœ,œidœ:œParseData-MJ5EYœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","animated":false,"className":""},{"source":"ParseData-MJ5EY","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-MJ5EYœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-oCI53","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-oCI53œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"Prompt-oCI53","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-MJ5EY","name":"text","output_types":["Message"]}},"id":"reactflow__edge-ParseData-MJ5EY{œdataTypeœ:œParseDataœ,œidœ:œParseData-MJ5EYœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-oCI53{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-oCI53œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-oCI53","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-oCI53œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"GoogleGenerativeAIModel-lXKNn","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-lXKNnœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"GoogleGenerativeAIModel-lXKNn","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-oCI53","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-oCI53{œdataTypeœ:œPromptœ,œidœ:œPrompt-oCI53œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-lXKNn{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-lXKNnœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"GoogleGenerativeAIModel-lXKNn","sourceHandle":"{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-lXKNnœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-ToGpZ","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ToGpZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-ToGpZ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-lXKNn","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-GoogleGenerativeAIModel-lXKNn{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-lXKNnœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-ToGpZ{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ToGpZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""}],"viewport":{"x":343.546114733256,"y":193.23460846280318,"zoom":0.3201045922349672}},"updated_at":"2024-12-12T02:57:42+00:00","description":"Language Models, Mapped and Mastered.","user_id":"ddd33462-4310-41af-903d-0d3ada680315","tags":null}